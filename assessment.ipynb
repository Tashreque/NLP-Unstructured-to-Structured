{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import spacy\n",
    "import json\n",
    "import re\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class to preprocess text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocess_text:\n",
    "    def __init__(self, text_list) -> None:\n",
    "        self.preprocessed = self.get_lemmatized(\n",
    "            self.to_lowercase(\n",
    "                self.remove_stopwords(\n",
    "                    self.remove_punc(text_list))))\n",
    "\n",
    "    # REMOVE PUNCTUATION\n",
    "    def remove_punc(self, body_texts):\n",
    "        X_no_punc = []\n",
    "        for sentence in body_texts:\n",
    "            # Remove all punctuations except for '.'\n",
    "            remove = string.punctuation.replace('.', '')\n",
    "            remove = remove.replace('/', '')\n",
    "            remove = remove.replace('-', '')\n",
    "            remove = remove.replace(':', '')\n",
    "            X_no_punc.append(sentence.translate(\n",
    "                sentence.maketrans('', '', remove)))\n",
    "        return X_no_punc\n",
    "\n",
    "    # REMOVE STOPWORDS\n",
    "    def remove_stopwords(self, text_list):\n",
    "        # Removing stopwords from dataset.\n",
    "        # Tokenizing data set.\n",
    "        X_tokenized = []\n",
    "        for sentence in text_list:\n",
    "            X_tokenized.append(word_tokenize(sentence))\n",
    "\n",
    "        X_no_stopwords = []\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        for word_list in X_tokenized:\n",
    "            filtered_words = [\n",
    "                word for word in word_list if word.lower() not in stop_words]\n",
    "            X_no_stopwords.append(\" \".join(filtered_words))\n",
    "        return X_no_stopwords\n",
    "\n",
    "    # CONVERT TO LOWERCASE\n",
    "    def to_lowercase(self, text_list):\n",
    "        X_lower = []\n",
    "        for sentence in text_list:\n",
    "            X_lower.append(sentence.lower())\n",
    "        return X_lower\n",
    "\n",
    "    # LEMMATISATION\n",
    "    def get_lemmatized(self, text_list):\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        X_preprocessed = []\n",
    "        for sentence in text_list:\n",
    "            tokens = word_tokenize(sentence)\n",
    "            lemmatized_sentence = \" \".join(\n",
    "                [lemmatizer.lemmatize(word) for word in tokens])\n",
    "            X_preprocessed.append(lemmatized_sentence)\n",
    "        return X_preprocessed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This function reads an OCR obtained .txt file and returns a list of the\n",
    "lines in the file. It also gets rid of any blank lines.\n",
    "'''\n",
    "def read_file(file_name):\n",
    "    lines = []\n",
    "    with open(file_name, 'r') as file:\n",
    "        for line in file:\n",
    "            # Remove whitespaces on either end\n",
    "            stripped_line = line.strip()\n",
    "\n",
    "            # Only append lines which are not blank\n",
    "            if len(stripped_line) > 0:\n",
    "                lines.append(stripped_line)\n",
    "\n",
    "    return lines\n",
    "\n",
    "'''\n",
    "This function reads the X1.json file and forms a dictionary where the keys\n",
    "are the abbreviations and the values are sets of synonyms.\n",
    "'''\n",
    "def load_x1(file_path):\n",
    "    with open(file_path, \"r\") as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    parameter_dictionary = {}\n",
    "\n",
    "    for item in data:\n",
    "        abb = item[\"Abbreviation\"].lower()\n",
    "\n",
    "        # Conversion to set so that searching is more efficient (constant time).\n",
    "        syns = set([each.lower() for each in item[\"Synonyms\"]])\n",
    "\n",
    "        if abb not in parameter_dictionary:\n",
    "            parameter_dictionary[abb] = syns\n",
    "\n",
    "    return parameter_dictionary\n",
    "\n",
    "'''\n",
    "This function checks whether a given parameter is valid by using the X1.json\n",
    "file.\n",
    "'''\n",
    "def is_valid_parameter(param, parameter_dict):\n",
    "    # Check if parameter is a key in the dictionary formed from X1.\n",
    "    if param in parameter_dict:\n",
    "        return True\n",
    "    else:\n",
    "        # Check if parameter is in the set of synonyms for each key.\n",
    "        for key in parameter_dict:\n",
    "            if (param in parameter_dict[key]):\n",
    "                return True\n",
    "        return False\n",
    "    \n",
    "'''\n",
    "This function checks whether a number is a valid value.\n",
    "'''\n",
    "def validate_number(item, splits, index):\n",
    "    try:\n",
    "        # If number is valid or if the number is part of a range\n",
    "        number = float(item)\n",
    "        if index == len(splits) - 1:\n",
    "            if \"-\" in splits[index - 1][-1]:\n",
    "                return False\n",
    "        elif index == 0:\n",
    "            if \"-\" in splits[index + 1][0]:\n",
    "                return False\n",
    "        elif (\"(\" in splits[index - 1][-1]) and (\")\" in splits[index + 1][0]):\n",
    "                return False\n",
    "        else:\n",
    "            if (\"-\" in splits[index - 1][-1]) or (\"-\" in splits[index + 1][0]):\n",
    "                return False\n",
    "            else:\n",
    "                return True\n",
    "    except:\n",
    "        # If number cannot be parsed\n",
    "        return False\n",
    "\n",
    "'''\n",
    "Function to extract data from the unstructured text.\n",
    "'''\n",
    "def extract_data(lines, parameter_dict):\n",
    "    lod = []\n",
    "    for line in lines:\n",
    "        splits = line.split()\n",
    "        if len(splits) > 0:\n",
    "            if is_valid_parameter(splits[0], parameter_dict):\n",
    "\n",
    "                # Obtaining parameter\n",
    "                parameter = splits[0]\n",
    "\n",
    "                # Choosing the latest value\n",
    "                value = \"\"\n",
    "                i = len(splits) - 1\n",
    "                for item in reversed(splits):\n",
    "                    if validate_number(item, splits, i):\n",
    "                        value = item\n",
    "                        break\n",
    "                    i -= 1\n",
    "\n",
    "                # Initialize unit to \"\"\n",
    "                unit = \"\"\n",
    "                for split in splits:\n",
    "                    if '/' in split:\n",
    "                        unit = split\n",
    "\n",
    "                # Add dictionary to list if a value exists\n",
    "                if value != \"\":\n",
    "                    lod.append({\"parameter\": parameter,\n",
    "                                \"value\": value,\n",
    "                                \"unit\": unit})\n",
    "    return lod\n",
    "\n",
    "def make_structured(file_name):\n",
    "    # Obtain lines from file\n",
    "    lines = read_file(file_name)\n",
    "\n",
    "    # Retrieve the parameter dictionary from X1.json\n",
    "    param_dict = load_x1(\"X1.json\")\n",
    "\n",
    "    # Perform preprocessing like lowercasing, punctuation and stopword removal etc. \n",
    "    preprocessed_lines = Preprocess_text(lines).preprocessed\n",
    "\n",
    "    # Obtain list of dictionaries\n",
    "    lod = extract_data(preprocessed_lines, param_dict)\n",
    "    return lod\n",
    "                    \n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test approach for all text files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_files = glob.glob(\"*.txt\")\n",
    "lod_list = []\n",
    "for f in txt_files:\n",
    "    lod_list.append(make_structured(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'parameter': 'rbc', 'value': '4.48', 'unit': '5.50x10l4/l'},\n",
       " {'parameter': 'hct', 'value': '0.43', 'unit': ''},\n",
       " {'parameter': 'mcv', 'value': '91', 'unit': ''},\n",
       " {'parameter': 'mch', 'value': '28.1', 'unit': ''},\n",
       " {'parameter': 'mchc', 'value': '309', 'unit': 'g/l'},\n",
       " {'parameter': 'rdw', 'value': '12.9', 'unit': ''},\n",
       " {'parameter': 'esr', 'value': '21', 'unit': '/h'},\n",
       " {'parameter': 'sodium', 'value': '138', 'unit': '/l'},\n",
       " {'parameter': 'potassium', 'value': '4.5', 'unit': '/l'},\n",
       " {'parameter': 'chloride', 'value': '105', 'unit': '/l'},\n",
       " {'parameter': 'bicarbonate', 'value': '24', 'unit': '/l'},\n",
       " {'parameter': 'urea', 'value': '4.2', 'unit': '/l'},\n",
       " {'parameter': 'creatinine', 'value': '45', 'unit': '/l'},\n",
       " {'parameter': 'egfr', 'value': '59', 'unit': 'ml/min/1.73m2'},\n",
       " {'parameter': 'albumin', 'value': '37', 'unit': 'g/l'},\n",
       " {'parameter': 'alp', 'value': '68', 'unit': 'u/l'},\n",
       " {'parameter': 'ggt', 'value': '14', 'unit': 'u/l'},\n",
       " {'parameter': 'ast', 'value': '41', 'unit': 'u/l'},\n",
       " {'parameter': 'alt', 'value': '15', 'unit': 'u/l'},\n",
       " {'parameter': 'chol/hdl', 'value': '5.6', 'unit': 'chol/hdl'},\n",
       " {'parameter': 'transferrin', 'value': '2.56', 'unit': 'g/l'},\n",
       " {'parameter': 'ferritin', 'value': '28', 'unit': '/l'},\n",
       " {'parameter': 'folate', 'value': '54.5', 'unit': 'nmol/l'},\n",
       " {'parameter': 'tsh', 'value': '2.30', 'unit': '4.00miu/l'}]"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lod_list[1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
